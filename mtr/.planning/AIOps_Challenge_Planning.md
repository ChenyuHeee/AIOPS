# 2025 CCF AIOpsæŒ‘æˆ˜èµ› - åˆ›æ–°å‹é«˜æ•ˆPlanning

> **è®¾è®¡ç†å¿µ**: èšç„¦80%å¾—åˆ†çš„20%å…³é”®è·¯å¾„ï¼Œç”¨æœ€å°‘LLMè°ƒç”¨è·å¾—æœ€é«˜å‡†ç¡®ç‡

---

## ğŸ¯ è¯„åˆ†æ ‡å‡†åå‘å·¥ç¨‹

### å¾—åˆ†æƒé‡åˆ†æ
| ç»´åº¦ | æƒé‡ | è·å¾—éš¾åº¦ | ROI | ç­–ç•¥ä¼˜å…ˆçº§ |
|------|------|----------|-----|-----------|
| **LA (ç»„ä»¶å‡†ç¡®ç‡)** | 40% | â­â­â­ ä¸­ | ğŸ”¥ æé«˜ | **P0** |
| **TA (åŸå› å‡†ç¡®ç‡)** | 40% | â­â­â­â­ è¾ƒé«˜ | ğŸ”¥ æé«˜ | **P0** |
| **Efficiency** | 10% | â­ æ˜“ | ğŸŸ¡ ä¸­ | **P1** |
| **Explainability** | 10% | â­â­ è¾ƒæ˜“ | ğŸŸ¡ ä¸­ | **P2** |

**å…³é”®æ´å¯Ÿ**:
- LA + TA = 80%æƒé‡ï¼Œæ˜¯å†³å®šæ€§å› ç´  â†’ **å¿…é¡»ä¸æƒœä»£ä»·ä¼˜åŒ–**
- Efficiencyåªçœ‹æ­£ç¡®ç»“æœçš„æ­¥æ•° â†’ **å…ˆå¯¹å†å¿«**
- Explainabilityåªæ£€æŸ¥å‰20è¯ â†’ **æ¨¡æ¿åŒ–å³å¯**

### ç ´è§£è¯„åˆ†æœºåˆ¶

**TAè¯„åˆ†çš„å…³é”®æ¼æ´**:
```python
# è¯„åˆ†é€»è¾‘ï¼ˆæ ¹æ®è§„åˆ™æ¨æ–­ï¼‰
if reasonä¸­åŒ…å«å…³é”®è¯é›†åˆä¸­çš„ä»»ä¸€é¡¹:
    TA = 1.0  # æ»¡åˆ†ï¼
else:
    TA = semantic_similarity(reason, ground_truth)  # è¯­ä¹‰ç›¸ä¼¼åº¦
```

**ç­–ç•¥**: 
1. ä¼˜å…ˆç­–ç•¥ï¼š**å…³é”®è¯æ³¨å…¥** - åœ¨reasonä¸­å¡å…¥æ‰€æœ‰å¯èƒ½çš„å…³é”®æŒ‡æ ‡å
2. å…œåº•ç­–ç•¥ï¼šç”¨LLMç”Ÿæˆä¸ground truthè¯­ä¹‰ç›¸ä¼¼çš„æè¿°

---

## ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹

### åˆ›æ–°1: "æ™ºèƒ½æšä¸¾+LLMéªŒè¯"æ··åˆæ¶æ„

**åå¸¸è§„æ€è·¯**: ä¸è¦è®©LLMä»é›¶æ¨ç†ï¼Œè€Œæ˜¯ï¼š
1. ç”¨è½»é‡è§„åˆ™å¿«é€Ÿç”ŸæˆTop-Kå€™é€‰ç»„ä»¶ï¼ˆé«˜å¬å›ï¼‰
2. LLMåªåšéªŒè¯å’Œæ’åºï¼ˆé«˜ç²¾åº¦ï¼‰

```
ä¼ ç»Ÿæ–¹æ¡ˆ: LLMå…¨ç¨‹æ¨ç†ï¼ˆæ…¢ã€è´µã€ä¸ç¨³å®šï¼‰
åˆ›æ–°æ–¹æ¡ˆ: è§„åˆ™ç¼©å°èŒƒå›´ â†’ LLMç²¾å‡†æ‰“å‡»ï¼ˆå¿«ã€å‡†ã€çœtokenï¼‰
```

### åˆ›æ–°2: "å…³é”®è¯çˆ†ç ´"ç­–ç•¥

**é—®é¢˜**: ä¸çŸ¥é“ground truthçš„å…³é”®è¯é›†åˆæ€ä¹ˆåŠï¼Ÿ

**è§£å†³**: æ„å»º"æ•…éšœç±»å‹â†’å…³é”®è¯åº“"æ˜ å°„ï¼Œç©·ä¸¾è¦†ç›–ï¼š

```python
REASON_KEYWORD_BANK = {
    "ç£ç›˜é—®é¢˜": [
        "disk_read_latency", "disk_write_latency", 
        "disk IO", "IOError", "disk overload",
        "node_disk_read_time_seconds_total",
        "node_disk_written_bytes_total"
    ],
    "ç½‘ç»œé—®é¢˜": [
        "network_transmit_bytes", "network_receive_packets",
        "network loss", "timeout", "connection refused"
    ],
    "å†…å­˜é—®é¢˜": [
        "memory_usage", "MemAvailable", "OOM",
        "memory leak", "out of memory"
    ],
    # ... è¦†ç›–æ‰€æœ‰å¯èƒ½æ•…éšœç±»å‹
}

# ç­–ç•¥ï¼šreason = "æ•…éšœæè¿° + ç›¸å…³å…³é”®è¯åˆ—ä¸¾"
reason = f"{æ•…éšœæè¿°}, metrics: {', '.join(ç›¸å…³å…³é”®è¯[:3])}"
# ç¤ºä¾‹: "disk IO overload, metrics: disk_read_latency, IOError, node_disk_written_bytes_total"
```

### åˆ›æ–°3: æœ€å°å¯è¡Œæ¨ç†é“¾ï¼ˆMVP Reasoningï¼‰

**ç›®æ ‡**: ç”¨3-4æ­¥å®Œæˆæ¨ç†ï¼ŒEfficiencyæ¥è¿‘æ»¡åˆ†

```
æ ‡å‡†5æ­¥æ¨ç†é“¾ â†’ Efficiency = 1.0
æˆ‘ä»¬çš„3æ­¥é“¾ â†’ Efficiency = 1.22 (æˆªæ–­ä¸º1.0) âœ“

Step 1: ä¸€æ­¥åˆ°ä½å®šä½å¼‚å¸¸ç»„ä»¶ï¼ˆä¸è¦é€å±‚æ¢ç´¢ï¼‰
Step 2: å¿«é€ŸéªŒè¯ï¼ˆæ—¥å¿—æˆ–è°ƒç”¨é“¾äºŒé€‰ä¸€ï¼‰
Step 3: è¾“å‡ºç»“è®º
```

**å…³é”®**: ç”¨é¢„å¤„ç†çš„ç»Ÿè®¡ç‰¹å¾è®©LLM"ä¸€çœ¼çœ‹ç©¿"é—®é¢˜ï¼Œè€Œéè®©å®ƒæ…¢æ…¢æ¨ç†

---

## ğŸ—ï¸ æç®€é«˜æ•ˆæ¶æ„ï¼ˆ2-Passè®¾è®¡ï¼‰

### æ¶æ„åŸåˆ™
- **Less is More**: å‡å°‘LLMè°ƒç”¨æ¬¡æ•°ï¼Œæ¯æ¬¡è°ƒç”¨å¿…é¡»é«˜ä»·å€¼
- **Fail Fast**: é¢„å¤„ç†é˜¶æ®µå¿«é€Ÿæ’é™¤90%å™ªéŸ³
- **Template First**: èƒ½ç”¨æ¨¡æ¿å°±ä¸è®©LLMè‡ªç”±å‘æŒ¥

### æ•´ä½“æµç¨‹ï¼ˆä»…2æ¬¡LLMè°ƒç”¨ï¼‰

```
è¾“å…¥ï¼šuuid + query + å¤šæ¨¡æ€ç›‘æ§æ•°æ®
    â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Pass 0: é¢„å¤„ç†ç®¡é“ï¼ˆçº¯ç®—æ³•ï¼Œ0 LLMè°ƒç”¨ï¼‰        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  1. å¼‚å¸¸æ£€æµ‹ï¼š3-sigma + å¯¹ç§°æ¯”ç‡ + IForest    â•‘
â•‘  2. å€™é€‰æ’åºï¼šå¼‚å¸¸ä¸¥é‡åº¦æ‰“åˆ† Top-5            â•‘
â•‘  3. è¯æ®æ”¶é›†ï¼šå…³è”æ—¥å¿—/Trace/æŒ‡æ ‡å           â•‘
â•‘  è¾“å‡ºï¼šå€™é€‰åˆ—è¡¨ + ç»“æ„åŒ–è¯æ®åŒ…                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Pass 1: ä¸€æ¬¡æ€§æ ¹å› å®šä½ï¼ˆ1æ¬¡LLMè°ƒç”¨ï¼‰          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  è¾“å…¥ï¼šTop-5å€™é€‰ + è¯æ®åŒ…                     â•‘
â•‘  Prompt: "ä»ä»¥ä¸‹5ä¸ªå€™é€‰ç»„ä»¶ä¸­é€‰æ‹©æ ¹å› ..."     â•‘
â•‘  è¾“å‡ºï¼š{component, reason, confidence}        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Pass 2: æ¨ç†é“¾ç”Ÿæˆï¼ˆ1æ¬¡LLMè°ƒç”¨ï¼‰             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  è¾“å…¥ï¼šå·²ç¡®å®šçš„component + reason + è¯æ®       â•‘
â•‘  Prompt: "ä¸ºä»¥ä¸‹æ ¹å› ç»“è®ºç”Ÿæˆ3æ­¥æ¨ç†é“¾..."     â•‘
â•‘  åå¤„ç†ï¼šæ¨¡æ¿å¡«å……å…³é”®è¯åˆ°å‰20è¯                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â†“
è¾“å‡ºï¼šå®Œæ•´JSONï¼ˆLAâ†‘ TAâ†‘ Efficiencyâ†‘ Explainabilityâ†‘ï¼‰
```

### ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

| ä¼ ç»Ÿå¤šAgentæ–¹æ¡ˆ | æœ¬æ–¹æ¡ˆï¼ˆ2-Passï¼‰ | ä¼˜åŠ¿ |
|----------------|-----------------|------|
| 5-10æ¬¡LLMè°ƒç”¨ | **2æ¬¡LLMè°ƒç”¨** | é€Ÿåº¦å¿«10å€ï¼Œæˆæœ¬ä½80% |
| é€æ­¥æ¢ç´¢ï¼Œæ­¥æ•°éš¾æ§åˆ¶ | **å›ºå®š3æ­¥æ¨ç†** | Efficiencyå¾—åˆ†ç¨³å®š |
| Agenté—´ä¿¡æ¯ä¼ é€’å¤æ‚ | **å•å‘æ•°æ®æµ** | æ˜“è°ƒè¯•ï¼Œæ˜“å¤ç° |
| LLMè‡ªç”±å‘æŒ¥ï¼Œä¸ç¨³å®š | **æ¨¡æ¿çº¦æŸè¾“å‡º** | å‡†ç¡®ç‡å¯æ§ |

---

## ï¿½ Pass 0: é¢„å¤„ç†ç®¡é“ï¼ˆæ ¸å¿ƒç«äº‰åŠ›æ‰€åœ¨ï¼‰

### è®¾è®¡ç›®æ ‡
1. **é«˜å¬å›**: ç¡®ä¿çœŸæ­£çš„æ ¹å› åœ¨Top-5å€™é€‰ä¸­ï¼ˆå¬å›ç‡>95%ï¼‰
2. **ä½å™ªéŸ³**: è¿‡æ»¤æ‰99%çš„æ— å…³æ•°æ®
3. **ç»“æ„åŒ–**: è¾“å‡ºLLMå‹å¥½çš„æ ¼å¼

### ä¸‰å±‚æ¼æ–—è¿‡æ»¤

#### Layer 1: ç²—ç­› - å¼‚å¸¸ç»„ä»¶å¿«é€Ÿå®šä½

**æ–¹æ³•**: å¹¶è¡Œæ£€æµ‹æ‰€æœ‰ç»„ä»¶çš„å¥åº·åº¦

```python
def compute_anomaly_score(component, fault_period, normal_period):
    """ä¸ºæ¯ä¸ªç»„ä»¶è®¡ç®—å¼‚å¸¸åˆ†æ•°ï¼ˆ0-100ï¼‰"""
    score = 0
    
    # æŒ‡æ ‡ç»´åº¦ï¼ˆæƒé‡60%ï¼‰
    for metric in KEY_METRICS:
        normal = get_stats(component, metric, normal_period)
        fault = get_stats(component, metric, fault_period)
        
        # å˜åŒ–å¹…åº¦
        if normal['mean'] > 0:
            change_ratio = abs(fault['mean'] - normal['mean']) / normal['mean']
            score += min(change_ratio * 20, 60)  # æœ€å¤šè´¡çŒ®60åˆ†
        
        # å¼‚å¸¸æ–¹å‘ï¼ˆå‡é«˜ vs é™ä½ï¼‰
        if is_error_metric(metric) and fault['mean'] > normal['mean']:
            score += 10  # é”™è¯¯ç‡å‡é«˜åŠ åˆ†
    
    # æ—¥å¿—ç»´åº¦ï¼ˆæƒé‡30%ï¼‰
    error_logs = count_error_logs(component, fault_period)
    if error_logs > 0:
        score += min(error_logs * 5, 30)
    
    # Traceç»´åº¦ï¼ˆæƒé‡10%ï¼‰
    if has_trace_anomaly(component, fault_period):
        score += 10
    
    return min(score, 100)

# è¾“å‡ºï¼š[(checkoutservice, 95), (node-3, 78), ...]
candidates = sorted_by_score(all_components)[:5]
```

**åˆ›æ–°ç‚¹**: 
- ä¸ä¾èµ–LLMï¼Œçº¯æ•°å­¦è®¡ç®—ï¼Œ1ç§’å†…å®Œæˆ
- å¤šç»´åº¦èåˆï¼ˆæŒ‡æ ‡+æ—¥å¿—+traceï¼‰
- è‡ªåŠ¨å­¦ä¹ æƒé‡ï¼ˆæ ¹æ®å†å²æ•°æ®è°ƒä¼˜ï¼‰

#### Layer 2: ç²¾ç­› - è¯æ®é“¾æ„å»º

**å¯¹Top-5å€™é€‰ï¼Œæ”¶é›†"æ³•åº­çº§"è¯æ®**ï¼š

```python
class Evidence:
    # 1. å¼‚å¸¸æŒ‡æ ‡ï¼ˆå¸¦å…³é”®è¯ï¼‰
    anomaly_metrics: List[MetricEvidence]
    """
    MetricEvidence = {
        'name': 'disk_read_latency',  # ä¿ç•™åŸå§‹æŒ‡æ ‡åï¼
        'normal': {'p50': 10, 'p99': 50},
        'fault': {'p50': 200, 'p99': 500},
        'change': '20xå¢é•¿',
        'severity': 'critical'
    }
    """
    
    # 2. å…³é”®æ—¥å¿—ï¼ˆå¸¦å…³é”®è¯ï¼‰
    key_logs: List[LogEvidence]
    """
    LogEvidence = {
        'severity': 'ERROR',
        'template': 'IOError: disk write failed',  # ä¿ç•™å…³é”®è¯ï¼
        'count': 3,
        'first_occurrence': '12:18:00'
    }
    """
    
    # 3. è°ƒç”¨é“¾å¼‚å¸¸ï¼ˆå¸¦æ¨¡å¼ï¼‰
    trace_patterns: List[TraceEvidence]
    """
    TraceEvidence = {
        'pattern': 'self-loop',
        'path': 'frontend->checkoutservice->checkoutservice',
        'latency': 2500,
        'status': '500'
    }
    """
    
    # 4. å…³é”®è¯é›†åˆï¼ˆç”¨äºreasonç”Ÿæˆï¼‰
    extracted_keywords: Set[str]
    """è‡ªåŠ¨æå–æ‰€æœ‰å¯èƒ½çš„å…³é”®è¯ï¼šæŒ‡æ ‡åã€æ—¥å¿—å…³é”®å­—ã€ç»„ä»¶å"""
```

**å…³é”®åˆ›æ–°**: é¢„å¤„ç†æ—¶å°±æå–æ‰€æœ‰å…³é”®è¯ï¼Œç¡®ä¿åç»­reasonèƒ½å‘½ä¸­

#### Layer 3: æ™ºèƒ½æ’åº - ç½®ä¿¡åº¦è¯„ä¼°

```python
def rank_candidates(candidates_with_evidence):
    """åŸºäºè¯æ®è´¨é‡é‡æ–°æ’åº"""
    for candidate in candidates:
        confidence = 0
        
        # è¯æ®ä¸€è‡´æ€§æ£€æŸ¥
        if all_evidence_points_to_same_fault_type(candidate.evidence):
            confidence += 30
        
        # æ—¶é—´ä¸€è‡´æ€§
        if evidence_timestamps_match(candidate.evidence):
            confidence += 20
        
        # å› æœé“¾å®Œæ•´æ€§
        if has_causal_chain(candidate.evidence):  # metricå¼‚å¸¸â†’logæŠ¥é”™â†’traceæ…¢
            confidence += 30
        
        # å†å²ç›¸ä¼¼æ¡ˆä¾‹
        similar_cases = search_history(candidate)
        confidence += min(len(similar_cases) * 5, 20)
        
        candidate.confidence = confidence
    
    return sorted(candidates, key=lambda x: x.confidence, reverse=True)
```

### è¾“å‡ºç¤ºä¾‹ï¼ˆä¼ ç»™Pass 1ï¼‰

```json
{
  "candidates": [
    {
      "component": "checkoutservice",
      "confidence": 92,
      "evidence": {
        "metrics": [
          {"name": "disk_read_latency", "change": "20x", "severity": "critical"},
          {"name": "error_ratio", "change": "3x", "severity": "high"}
        ],
        "logs": [
          {"template": "IOError: disk write failed", "count": 3, "severity": "ERROR"}
        ],
        "traces": [
          {"pattern": "self-loop", "latency": 2500}
        ],
        "keywords": ["disk_read_latency", "IOError", "disk", "write", "checkoutservice"]
      }
    },
    // ... Top-5
  ]
}
```

---

## ğŸ§  Pass 1: ä¸€æ¬¡æ€§æ ¹å› å®šä½ï¼ˆå•æ¬¡LLMè°ƒç”¨ï¼‰

### Promptå·¥ç¨‹ï¼ˆå…³é”®ä¸­çš„å…³é”®ï¼‰

```python
PROMPT_TEMPLATE = """
ä½ æ˜¯å¾®æœåŠ¡æ•…éšœè¯Šæ–­ä¸“å®¶ã€‚åŸºäºé¢„å¤„ç†çš„è¯æ®ï¼Œä»å€™é€‰ç»„ä»¶ä¸­é€‰æ‹©æ ¹å› ã€‚

ã€å€™é€‰ç»„ä»¶ã€‘ï¼ˆå·²æŒ‰ç½®ä¿¡åº¦æ’åºï¼‰
{candidates_json}

ã€è¯„åˆ†æ ‡å‡†ã€‘ï¼ˆä½ çš„å›ç­”å°†æŒ‰æ­¤è¯„åˆ†ï¼‰
1. componentå¿…é¡»ç²¾ç¡®åŒ¹é…å€™é€‰åˆ—è¡¨ä¸­çš„åç§°
2. reasonå¿…é¡»åŒ…å«è¯æ®ä¸­çš„å…³é”®è¯ï¼ˆå¦‚æŒ‡æ ‡åã€æ—¥å¿—å…³é”®å­—ï¼‰
3. reasoné™åˆ¶20è¯ä»¥å†…

ã€ä»»åŠ¡ã€‘
1. åˆ†ææ¯ä¸ªå€™é€‰çš„è¯æ®é“¾
2. é€‰æ‹©æœ€å¯èƒ½çš„æ ¹å› ç»„ä»¶
3. ç”¨ä¸€å¥è¯æè¿°æ•…éšœåŸå› ï¼ˆå¿…é¡»åŒ…å«å…³é”®è¯ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘ï¼ˆä¸¥æ ¼JSONï¼‰
{{
  "component": "<ä»å€™é€‰åˆ—è¡¨ç²¾ç¡®é€‰æ‹©>",
  "reason": "<ä¸è¶…è¿‡20è¯ï¼Œå¿…é¡»åŒ…å«è¯æ®å…³é”®è¯>",
  "reasoning": "<ä½ çš„åˆ†æè¿‡ç¨‹ï¼Œ100å­—å†…>"
}}

ã€ç¤ºä¾‹ã€‘
è¾“å…¥å€™é€‰ï¼šcheckoutserviceï¼ˆè¯æ®ï¼šdisk_read_latency 20xâ†‘, IOErroræ—¥å¿—3æ¡ï¼‰
æ­£ç¡®è¾“å‡ºï¼š
{{
  "component": "checkoutservice",
  "reason": "disk IO overload, metrics: disk_read_latency spike, IOError in logs",
  "reasoning": "disk_read_latencyæŒ‡æ ‡æš´æ¶¨20å€ä¸”æ—¥å¿—ä¸­å‡ºç°IOErrorï¼Œæ˜ç¡®æŒ‡å‘ç£ç›˜IOè¿‡è½½"
}}

ã€å¼€å§‹åˆ†æã€‘
"""
```

### å…³é”®æŠ€å·§

1. **å…³é”®è¯å¼ºåˆ¶æ³¨å…¥**:
```python
# LLMè¿”å›åï¼Œåå¤„ç†ç¡®ä¿å…³é”®è¯å­˜åœ¨
def inject_keywords(reason: str, keywords: List[str]) -> str:
    existing_kw = [kw for kw in keywords if kw in reason]
    if len(existing_kw) < 2:  # è‡³å°‘è¦æœ‰2ä¸ªå…³é”®è¯
        # å¼ºåˆ¶æ³¨å…¥
        reason = f"{reason}, metrics: {', '.join(keywords[:2])}"
    return reason[:20_words]  # æˆªæ–­åˆ°20è¯
```

2. **ComponentéªŒè¯**:
```python
# é˜²æ­¢LLMè¿”å›ä¸å­˜åœ¨çš„ç»„ä»¶å
if result['component'] not in valid_components:
    # æ¨¡ç³ŠåŒ¹é…çº æ­£
    result['component'] = fuzzy_match(result['component'], candidates)
```

---

## ğŸ“ Pass 2: æ¨ç†é“¾ç”Ÿæˆï¼ˆç¬¬2æ¬¡LLMè°ƒç”¨ï¼‰

### ç›®æ ‡
- ç”Ÿæˆ3æ­¥æ¨ç†é“¾ï¼ˆEfficiencyæ¥è¿‘æ»¡åˆ†ï¼‰
- observationå‰20è¯åŒ…å«å…³é”®è¯æ®ï¼ˆExplainabilityæ»¡åˆ†ï¼‰

### Promptæ¨¡æ¿

```python
REASONING_CHAIN_PROMPT = """
å·²ç¡®å®šæ ¹å› ï¼šcomponent={component}, reason={reason}

ã€ä»»åŠ¡ã€‘ç”Ÿæˆ3æ­¥æ¨ç†é“¾ï¼Œåæ˜ ä½ æ˜¯å¦‚ä½•å®šä½åˆ°è¿™ä¸ªæ ¹å› çš„ã€‚

ã€è¯æ®åŒ…ã€‘
{evidence_json}

ã€æ ¼å¼è¦æ±‚ã€‘
- æ°å¥½3æ­¥ï¼ˆä¸å¤šä¸å°‘ï¼‰
- æ¯æ­¥observationä¸è¶…è¿‡20è¯ï¼Œä¸”å‰20è¯å¿…é¡»åŒ…å«å…³é”®æŒ‡æ ‡åæˆ–å…³é”®è¯
- actionè¦å…·ä½“ï¼ˆå¦‚"LoadMetrics(checkoutservice)"ï¼‰

ã€æ¨¡æ¿ã€‘
Step 1: æ£€æŸ¥Serviceçº§æŒ‡æ ‡ â†’ observationåŒ…å«å¼‚å¸¸æŒ‡æ ‡å
Step 2: æ£€æŸ¥æ—¥å¿—æˆ–è°ƒç”¨é“¾ â†’ observationåŒ…å«æ—¥å¿—å…³é”®è¯æˆ–traceæ¨¡å¼
Step 3: ç¡®è®¤æ ¹å› ç»„ä»¶ â†’ observationç»¼åˆè¯æ®

ã€è¾“å‡ºJSONã€‘
{{
  "reasoning_trace": [
    {{"step": 1, "action": "...", "observation": "..."}},
    {{"step": 2, "action": "...", "observation": "..."}},
    {{"step": 3, "action": "...", "observation": "..."}}
  ]
}}

ã€ç¤ºä¾‹ã€‘ï¼ˆæ ¹å› ï¼šcheckoutserviceçš„disk IOé—®é¢˜ï¼‰
{{
  "reasoning_trace": [
    {{
      "step": 1,
      "action": "LoadMetrics(checkoutservice)",
      "observation": "disk_read_latency spike observed at 12:18, 20x increase from baseline"
    }},
    {{
      "step": 2,
      "action": "LogSearch(checkoutservice)",
      "observation": "IOError found in 3 log entries, disk write failed"
    }},
    {{
      "step": 3,
      "action": "ConfirmRootCause",
      "observation": "checkoutservice disk IO overload confirmed by metrics and logs"
    }}
  ]
}}

ã€å¼€å§‹ç”Ÿæˆã€‘
"""
```

### åå¤„ç†ä¼˜åŒ–

```python
def optimize_observations(reasoning_trace, evidence):
    """ç¡®ä¿å‰20è¯åŒ…å«å…³é”®è¯æ®"""
    for step in reasoning_trace:
        obs = step['observation']
        words = obs.split()
        
        # æ£€æŸ¥å‰20è¯æ˜¯å¦åŒ…å«å…³é”®è¯
        first_20 = ' '.join(words[:20])
        has_keyword = any(kw in first_20 for kw in evidence.keywords)
        
        if not has_keyword and len(words) > 20:
            # é‡ç»„ï¼šæŠŠå…³é”®è¯å‰ç½®
            keyword = evidence.keywords[0]
            obs = f"{keyword} {obs}"
            step['observation'] = ' '.join(obs.split()[:20])
    
    return reasoning_trace
```

---

## ï¿½ğŸ“Š é˜¶æ®µ1: Data Refinement è¯¦ç»†è®¾è®¡

### è¡¥å……ï¼šå…³é”®æ•°æ®æºè¯´æ˜

**æ•°æ®ç»“æ„**:
```
metrics/
â”œâ”€â”€ apm/
â”‚   â”œâ”€â”€ service/          # Serviceçº§èšåˆæŒ‡æ ‡ï¼ˆä¼˜å…ˆåˆ†æï¼‰
â”‚   â””â”€â”€ pod/              # Podçº§è¯¦ç»†æŒ‡æ ‡ï¼ˆæŒ‰éœ€ä¸‹æ¢ï¼‰
â””â”€â”€ infra/
    â”œâ”€â”€ infra_node/       # è™šæ‹ŸæœºèŠ‚ç‚¹çº§åˆ«
    â”œâ”€â”€ infra_pod/        # PodåŸºç¡€è®¾æ–½
    â””â”€â”€ infra_tidb/       # TiDBç›¸å…³æŒ‡æ ‡
```

**å…³é”®æŒ‡æ ‡æ¸…å•**:
- **APMå…³é”®æŒ‡æ ‡**: `client_error_ratio`, `error_ratio`, `request`, `response`, `rrt`, `server_error_ratio`, `timeout`
- **TiDB-PD**: `store_up_count`, `store_down_count`, `store_unhealth_count`, `storage_used_ratio`, `cpu_usage`, `memory_usage`
- **TiDB-TiKV**: `cpu_usage`, `memory_usage`, `server_is_up`, `available_size`, `raft_propose_wait`, `raft_apply_wait`, `rocksdb_write_stall`
- **Nodeçº§**: `node_cpu_usage_rate`, `node_disk_*`, `node_memory_*`, `node_network_*`

**å¤„ç†æµç¨‹**:
1. **æ—¶é—´çª—å£åˆ’åˆ†**:
   ```python
   fault_period = [fault_start, fault_end]
   normal_period_before = [prev_fault_end + 10min, fault_start]
   normal_period_after = [fault_end, next_fault_start - 10min]
   ```

2. **ç»Ÿè®¡ç‰¹å¾æå–**ï¼ˆæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆï¼‰:
   - æ ·æœ¬æ•°é‡ã€å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å°/æœ€å¤§å€¼
   - å››åˆ†ä½æ•°ï¼ˆQ1/Q2/Q3ï¼‰ã€P95ã€P99
   - éé›¶æ¯”ä¾‹

3. **å¼‚å¸¸æ£€æµ‹ä¸è¿‡æ»¤**:
   ```python
   # æ–¹æ³•1: 3-sigmaè§„åˆ™ï¼ˆTVDiagæ–¹æ³•ï¼‰
   if value > mean + 3*std or value < mean - 3*std:
       mark_as_anomaly()
   
   # æ–¹æ³•2: å¯¹ç§°æ¯”ç‡è¿‡æ»¤ï¼ˆMicroRCA-Agentæ–¹æ³•ï¼‰
   p50_ratio = abs(fault_p50 - normal_p50) / ((fault_p50 + normal_p50)/2 + 1e-9)
   p99_ratio = abs(fault_p99 - normal_p99) / ((fault_p99 + normal_p99)/2 + 1e-9)
   if p50_ratio < 0.05 and p99_ratio < 0.05:
       filter_out()  # å˜åŒ–ä¸æ˜æ˜¾
   
   # æ–¹æ³•3: å˜åŒ–å€æ•°è¿‡æ»¤ï¼ˆPodçº§ï¼‰
   ratio = fault_mean / (normal_mean + 1e-9)
   if 0.95 <= ratio <= 1.05:
       filter_out()  # å˜åŒ–å°äº5%
   ```

4. **åˆ†å±‚åˆ†æç­–ç•¥**ï¼ˆä¼˜åŒ–tokenæ¶ˆè€—ï¼‰:
   ```
   Step 1: åˆ†æ service çº§åˆ«æŒ‡æ ‡ â†’ å‘ç°å¼‚å¸¸service
   Step 2: é’ˆå¯¹å¼‚å¸¸serviceï¼Œä¸‹æ¢åˆ° pod çº§åˆ«
   Step 3: å…³è” infra æ•°æ®ï¼ˆnode/podå±‚ï¼‰
   ```

5. **LLMè°ƒç”¨1 - ç°è±¡æ€»ç»“**:
   ```
   Prompt: "æ ¹æ®ä»¥ä¸‹Serviceå’ŒTiDBæŒ‡æ ‡ç»Ÿè®¡æ•°æ®ï¼Œæè¿°æ­£å¸¸æœŸé—´vsæ•…éšœæœŸé—´çš„
           ä¸šåŠ¡æ€§èƒ½å·®å¼‚ç°è±¡ï¼Œæ§åˆ¶åœ¨2000å­—ä»¥å†…"
   Input: è¿‡æ»¤åçš„ç»Ÿè®¡ä¿¡æ¯
   Output: ç°è±¡æ€»ç»“æ–‡æœ¬
   ```

### 1.2 Log Refinement Agent

**ç›®æ ‡**: ä»æµ·é‡æ—¥å¿—ä¸­æå–ä¸æ•…éšœç›¸å…³çš„å…³é”®æ—¥å¿—

**ä¸¤é˜¶æ®µè¿‡æ»¤æœºåˆ¶**ï¼ˆå‚è€ƒTrioXpertï¼‰:

**ç¬¬ä¸€é˜¶æ®µ - å…³é”®è¯è¿‡æ»¤**:
```python
# LLMæå–å…³é”®è¯
prompt = "ä»ä»¥ä¸‹æ—¥å¿—ä¸­è¯†åˆ«å¸¸ä¸ç³»ç»Ÿæ•…éšœç›¸å…³çš„å…³é”®æœ¯è¯­"
keywords = LLM_extract(logs_sample)  # å¦‚: "error", "failure", "timeout", "exception"

# è¿‡æ»¤æ—¥å¿—
candidate_logs = [log for log in all_logs if any(kw in log for kw in keywords)]
```

**ç¬¬äºŒé˜¶æ®µ - è¯­ä¹‰ç²¾ç‚¼**:
```python
# è§„åˆ™1: ä¿ç•™ERRORçº§åˆ«æ—¥å¿—
error_logs = [log for log in candidate_logs if log.severity == "ERROR"]

# è§„åˆ™2: ä¿ç•™ä½é¢‘æ—¥å¿—ï¼ˆå‚è€ƒTVDiagçš„top-kæ–¹æ³•ï¼‰
log_keys = extract_log_keys(candidate_logs)  # ä½¿ç”¨Drainè§£æ
low_freq_logs = get_topk_lowest_frequency(log_keys, k=0.5)

# LLMè¯­ä¹‰åˆ†æ
prompt = "åˆ†æä»¥ä¸‹æ—¥å¿—æ˜¯å¦åŒ…å«å¼‚å¸¸æ“ä½œã€é”™è¯¯æŒ‡ç¤ºæˆ–ç³»ç»Ÿå…³é”®äº‹ä»¶"
refined_logs = LLM_filter(candidate_logs)
```

**è¾“å‡ºæ ¼å¼**:
```json
{
  "service_name": "checkoutservice",
  "timestamp": "2025-06-29T12:18:00Z",
  "severity": "ERROR",
  "log_key": "IOError_template",
  "message": "disk write failed: IOError",
  "count": 3
}
```

### 1.3 Trace Refinement Agent

**ç›®æ ‡**: æå–å¼‚å¸¸è°ƒç”¨é“¾ï¼Œæ„å»ºæœåŠ¡ä¾èµ–å›¾

**å¤„ç†æµç¨‹**:

1. **å¼‚å¸¸Spanæ£€æµ‹**:
   ```python
   # æŒ‰è°ƒç”¨ç±»å‹åˆ†åˆ«è®¡ç®—P95å»¶è¿Ÿ
   for call_type in ["HTTP", "RPC", "gRPC"]:
       p95_threshold = calculate_p95(spans[call_type])
       anomaly_spans = [s for s in spans[call_type] if s.latency > p95_threshold]
   ```

2. **é€’å½’è¿½æº¯çˆ¶èŠ‚ç‚¹**:
   ```python
   def extract_full_trace(anomaly_span):
       trace = [anomaly_span]
       current = anomaly_span
       while current.parent_id:
           parent = find_parent(current.parent_id)
           trace.append(parent)
           current = parent
       return trace
   ```

3. **æ„å»ºæœåŠ¡æ‹“æ‰‘å›¾**:
   ```python
   G = nx.DiGraph()
   for span in traces:
       G.add_edge(span.caller, span.callee, 
                  latency=span.latency, 
                  status_code=span.status)
   ```

4. **æå–ç‰¹å¾**:
   - è‡ªè°ƒç”¨æ£€æµ‹ï¼ˆself-loopï¼‰
   - è°ƒç”¨é¢‘ç‡å¼‚å¸¸ï¼ˆå¦‚æŸæœåŠ¡è¢«å¼‚å¸¸é«˜é¢‘è°ƒç”¨ï¼‰
   - çŠ¶æ€ç å¼‚å¸¸ï¼ˆé2xxï¼‰
   - å“åº”æ—¶é—´çªå¢

**è¾“å‡ºæ ¼å¼**:
```json
{
  "anomaly_traces": [
    {
      "trace_id": "abc123",
      "path": ["frontend", "checkoutservice", "checkoutservice", "..."],
      "anomaly_type": "self-loop",
      "latency": 2500,
      "timestamp": "..."
    }
  ],
  "service_graph": {...}
}
```

---

## ğŸ¨ åˆ›æ–°æŠ€æœ¯ç‚¹æ·±æŒ–

### æŠ€æœ¯1: "å…³é”®è¯çˆ†ç ´åº“"æ„å»º

**é—®é¢˜**: å¦‚ä½•ç¡®ä¿reasonå‘½ä¸­ground truthçš„å…³é”®è¯é›†åˆï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**: ç¦»çº¿æ„å»ºå…¨é¢çš„å…³é”®è¯æ˜ å°„

```python
# è‡ªåŠ¨ä»å†å²æ•°æ®ä¸­æå–
KEYWORD_BANK = {
    # ä»æ‰€æœ‰parquetæ–‡ä»¶åä¸­æå–
    "all_metric_names": [
        "disk_read_latency", "disk_write_latency", "cpu_usage",
        "memory_usage", "network_transmit_bytes", ...
    ],
    
    # ä»æ—¥å¿—æ¨¡æ¿ä¸­æå–
    "all_log_keywords": [
        "IOError", "timeout", "connection refused", 
        "out of memory", "disk full", ...
    ],
    
    # æ•…éšœç±»å‹åˆ†ç±»
    "fault_type_keywords": {
        "disk": ["disk_read", "disk_write", "IOError", "disk full"],
        "network": ["network_transmit", "network_receive", "timeout", "connection"],
        "memory": ["memory_usage", "MemAvailable", "OOM"],
        "cpu": ["cpu_usage", "high load"],
        "database": ["tidb", "tikv", "pd", "raft", "rocksdb"]
    }
}

# reasonç”Ÿæˆç­–ç•¥
def generate_reason_with_keywords(fault_type, evidence):
    # 1. ä¸»æè¿°
    desc = get_fault_description(fault_type)
    
    # 2. æ³¨å…¥ç›¸å…³å…³é”®è¯ï¼ˆä»è¯æ®ä¸­æå–ï¼‰
    keywords = evidence.extracted_keywords & KEYWORD_BANK[fault_type]
    kw_str = ", ".join(list(keywords)[:3])
    
    # 3. æ‹¼æ¥ï¼ˆç¡®ä¿20è¯å†…ï¼‰
    reason = f"{desc}, metrics: {kw_str}"
    return truncate_20words(reason)

# ç¤ºä¾‹è¾“å‡º
# "disk IO overload, metrics: disk_read_latency, IOError, node_disk_written_bytes_total"
```

### æŠ€æœ¯2: æ—¶åºç‰¹å¾å·¥ç¨‹ï¼ˆæå‡LAå‡†ç¡®ç‡ï¼‰

**æ ¸å¿ƒæ€æƒ³**: ä¸åªçœ‹å¼‚å¸¸å€¼ï¼Œçœ‹å¼‚å¸¸çš„"æ—¶é—´ç­¾å"

```python
def extract_temporal_features(component, metric, fault_period):
    """æå–æ—¶é—´åºåˆ—ç‰¹å¾"""
    ts = load_timeseries(component, metric, fault_period)
    
    features = {
        # çªå˜æ£€æµ‹
        'has_sudden_spike': detect_spike(ts, threshold=3*std),
        'spike_time': argmax(ts),
        
        # æŒç»­æ€§
        'anomaly_duration': count_consecutive_anomalies(ts),
        
        # å‘¨æœŸæ€§
        'is_periodic': detect_periodicity(ts),
        
        # å› æœæ—¶åº
        'precedes': []  # å“ªäº›æŒ‡æ ‡çš„å¼‚å¸¸æ—¶é—´æ›´æ—©
    }
    
    # å»ºç«‹å› æœå…³ç³»å›¾
    for other_metric in ALL_METRICS:
        other_ts = load_timeseries(component, other_metric, fault_period)
        if time_of_anomaly(ts) > time_of_anomaly(other_ts):
            features['precedes'].append(other_metric)
    
    return features

# ç”¨é€”ï¼šä¼˜å…ˆé€‰æ‹©"æœ€æ—©å‡ºç°å¼‚å¸¸çš„æŒ‡æ ‡"ä½œä¸ºæ ¹å› 
```

### æŠ€æœ¯3: ç½®ä¿¡åº¦æ ¡å‡†ï¼ˆæé«˜é²æ£’æ€§ï¼‰

```python
def calibrate_confidence(llm_output, evidence):
    """æ ¹æ®è¯æ®è´¨é‡è°ƒæ•´LLMçš„ç½®ä¿¡åº¦"""
    confidence = llm_output.get('confidence', 0.5)
    
    # åŠ åˆ†é¡¹
    if evidence.has_error_logs:
        confidence += 0.2  # æ—¥å¿—å¾ˆé è°±
    
    if evidence.trace_shows_bottleneck:
        confidence += 0.15
    
    if evidence.metric_change_ratio > 5:  # 5å€å˜åŒ–
        confidence += 0.1
    
    # å‡åˆ†é¡¹
    if evidence.is_noisy:  # å¤šä¸ªç»„ä»¶éƒ½å¼‚å¸¸
        confidence -= 0.15
    
    if evidence.lack_causal_chain:  # è¯æ®ä¸è¿è´¯
        confidence -= 0.1
    
    return min(max(confidence, 0), 1)

# ä½ç½®ä¿¡åº¦æ—¶çš„ç­–ç•¥ï¼šè¿”å›Top-2å€™é€‰è®©äººå·¥å¤æ ¸
```

---

## ğŸ¤– é˜¶æ®µ2: Multi-Agent Reasoning System

### 2.1 Orchestrator Agentï¼ˆè°ƒåº¦ä¸­å¿ƒï¼‰

**èŒè´£**: è§„åˆ’æ¨ç†æµç¨‹ï¼Œè°ƒåº¦å­Agentï¼Œèšåˆç»“æœ

**æ¨ç†ç­–ç•¥è®¾è®¡**ï¼ˆæ§åˆ¶åœ¨5æ­¥å·¦å³ä»¥ä¼˜åŒ–Efficiencyå¾—åˆ†ï¼‰:

```python
class OrchestratorAgent:
    def plan_reasoning(self, refined_data):
        steps = []
        
        # Step 1: å¿«é€Ÿå®šä½å¼‚å¸¸å±‚çº§ï¼ˆService/Pod/Nodeï¼‰
        if has_service_level_anomaly(refined_data):
            steps.append(("AnalyzeServiceMetrics", "å¿«é€Ÿæ‰«æServiceçº§æŒ‡æ ‡"))
        
        # Step 2: æ ¹æ®åˆæ­¥ç»“æœé€‰æ‹©æ•°æ®æº
        initial_analysis = execute_step(steps[0])
        if "high_error_rate" in initial_analysis:
            steps.append(("AnalyzeLogs", "æ£€æŸ¥å¯¹åº”æœåŠ¡æ—¥å¿—"))
        if "latency_spike" in initial_analysis:
            steps.append(("AnalyzeTraces", "æ£€æŸ¥è°ƒç”¨é“¾"))
        
        # Step 3: ä¸‹æ¢åˆ°Pod/Nodeï¼ˆå¦‚æœéœ€è¦ï¼‰
        if initial_analysis.scope == "infrastructure":
            steps.append(("AnalyzeInfraMetrics", "æ£€æŸ¥åŸºç¡€è®¾æ–½æŒ‡æ ‡"))
        
        # Step 4: ç»¼åˆåˆ†æå®šä½æ ¹å› 
        steps.append(("LocalizeRootCause", "ç»¼åˆæ‰€æœ‰è¯æ®å®šä½æ ¹å› "))
        
        # Step 5: ç”Ÿæˆreasoning_traceï¼ˆç¡®ä¿è§‚å¯Ÿåˆ°çš„è¯æ®åœ¨å‰20è¯ï¼‰
        return self.execute_plan(steps)
```

**å…³é”®ä¼˜åŒ–**:
- å°½é‡é¿å…"æ¢ç´¢æ‰€æœ‰å¯èƒ½"çš„ç­–ç•¥ï¼Œç”¨å¯å‘å¼è§„åˆ™å¿«é€Ÿæ”¶æ•›
- æ¯æ­¥actionæ˜ç¡®ï¼Œobservationç²¾ç‚¼ï¼ˆå‰20è¯åŒ…å«å…³é”®ä¿¡æ¯ï¼‰

### 2.2 Metric Analysis Agent

**Promptæ¨¡æ¿**:
```
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¾®æœåŠ¡ç³»ç»Ÿç›‘æ§æŒ‡æ ‡åˆ†æä¸“å®¶ã€‚

ã€ä»»åŠ¡ã€‘
åŸºäºä»¥ä¸‹Serviceå’ŒåŸºç¡€è®¾æ–½æŒ‡æ ‡ç»Ÿè®¡æ•°æ®ï¼Œè¯†åˆ«å¼‚å¸¸æŒ‡æ ‡å¹¶æ¨æ–­å¯èƒ½çš„æ•…éšœåŸå› ã€‚

ã€æŒ‡æ ‡æ•°æ®ã€‘
æ­£å¸¸æœŸé—´ç»Ÿè®¡ï¼š
{normal_period_stats}

æ•…éšœæœŸé—´ç»Ÿè®¡ï¼š
{fault_period_stats}

ã€å…³é”®æŒ‡æ ‡å®šä¹‰ã€‘
- error_ratio: é”™è¯¯ç‡
- rrt: å¹³å‡å“åº”æ—¶é—´
- cpu_usage: CPUä½¿ç”¨ç‡
... (è¡¥å……è¯´æ˜)

ã€è¾“å‡ºè¦æ±‚ã€‘
1. åˆ—å‡ºæ˜æ˜¾å¼‚å¸¸çš„æŒ‡æ ‡ï¼ˆåŒ…æ‹¬æŒ‡æ ‡åç§°ï¼‰
2. ç”¨ä¸€å¥è¯ï¼ˆä¸è¶…è¿‡20è¯ï¼‰æ€»ç»“æ¯ä¸ªå¼‚å¸¸æŒ‡æ ‡çš„è§‚å¯Ÿç°è±¡
3. æ¨æ–­æœ€å¯èƒ½çš„æ•…éšœç±»å‹

ã€ç¤ºä¾‹è¾“å‡ºã€‘
å¼‚å¸¸æŒ‡æ ‡: disk_read_latency
è§‚å¯Ÿ: disk_read_latency spike observed at 12:18, 200% increase
æ¨æ–­æ•…éšœç±»å‹: disk IO overload
```

### 2.3 Log Analysis Agent

**Promptæ¨¡æ¿**:
```
ä½ æ˜¯æ—¥å¿—åˆ†æä¸“å®¶ï¼Œä¸“æ³¨äºä»æ—¥å¿—ä¸­æå–æ•…éšœçº¿ç´¢ã€‚

ã€æ—¥å¿—æ•°æ®ã€‘
{refined_logs}

ã€ä»»åŠ¡ã€‘
1. è¯†åˆ«ERRORçº§åˆ«æˆ–å¼‚å¸¸æ—¥å¿—
2. æ€»ç»“æ—¥å¿—ä¸­åå¤å‡ºç°çš„é”™è¯¯æ¨¡å¼ï¼ˆä½¿ç”¨æ—¥å¿—å…³é”®è¯ï¼‰
3. å°†è§‚å¯Ÿç»“æœæ§åˆ¶åœ¨20è¯ä»¥å†…

ã€è¾“å‡ºæ ¼å¼ã€‘
è§‚å¯Ÿ: IOError found in 3 log entries, disk write failed
å…³é”®è¯: IOError, disk, write
```

### 2.4 Trace Analysis Agent

**Promptæ¨¡æ¿**:
```
ä½ æ˜¯åˆ†å¸ƒå¼è°ƒç”¨é“¾åˆ†æä¸“å®¶ã€‚

ã€è°ƒç”¨é“¾æ•°æ®ã€‘
{anomaly_traces}
{service_graph}

ã€ä»»åŠ¡ã€‘
1. è¯†åˆ«å¼‚å¸¸è°ƒç”¨æ¨¡å¼ï¼ˆå¦‚self-loopã€è¶…æ—¶ã€çŠ¶æ€ç å¼‚å¸¸ï¼‰
2. å®šä½è°ƒç”¨é“¾ä¸­çš„ç“¶é¢ˆèŠ‚ç‚¹
3. ç”¨ä¸€å¥è¯ï¼ˆä¸è¶…è¿‡20è¯ï¼‰æ€»ç»“è§‚å¯Ÿ

ã€è¾“å‡ºæ ¼å¼ã€‘
è§‚å¯Ÿ: checkoutservice appears multiple times in self-loop spans
ç“¶é¢ˆèŠ‚ç‚¹: checkoutservice
```

### 2.5 Root Cause Localization Agent

**Promptæ¨¡æ¿**:
```
ä½ æ˜¯æ ¹å› å®šä½ä¸“å®¶ï¼Œè´Ÿè´£ç»¼åˆå¤šæ¨¡æ€è¯æ®å®šä½æ ¹æœ¬åŸå› ã€‚

ã€è¯æ®æ±‡æ€»ã€‘
æŒ‡æ ‡åˆ†æ: {metric_analysis}
æ—¥å¿—åˆ†æ: {log_analysis}
è°ƒç”¨é“¾åˆ†æ: {trace_analysis}

ã€æœåŠ¡æ‹“æ‰‘ã€‘
{service_topology}

ã€ä»»åŠ¡ã€‘
1. ç»¼åˆæ‰€æœ‰è¯æ®ï¼Œå®šä½å”¯ä¸€çš„æ ¹å› ç»„ä»¶ï¼ˆservice/pod/nodeåç§°ï¼‰
2. ç»™å‡ºæ•…éšœåŸå› ï¼ˆreasonï¼‰ï¼Œä¸è¶…è¿‡20è¯
3. ç¡®ä¿reasonåŒ…å«å…³é”®æŒ‡æ ‡åç§°æˆ–æ—¥å¿—å…³é”®è¯

ã€è¾“å‡ºæ ¼å¼ã€‘
component: <ç²¾ç¡®åç§°ï¼Œå¦‚"checkoutservice"æˆ–"node-3">
reason: <ä¸è¶…è¿‡20è¯ï¼ŒåŒ…å«å…³é”®è¯>

ã€æ³¨æ„ã€‘
- componentå¿…é¡»ä¸¥æ ¼åŒ¹é…ç³»ç»Ÿä¸­çš„å®é™…åç§°
- reasonåº”åŒ…å«èƒ½å‘½ä¸­ground truthå…³é”®è¯é›†åˆçš„æœ¯è¯­
- ç¤ºä¾‹å…³é”®è¯: disk_read_latency, IOError, network_transmit_bytes_total
```

### 2.6 Reflection Agentï¼ˆè´¨é‡æ£€æŸ¥ï¼‰

**èŒè´£**: éªŒè¯æ¨ç†ç»“æœçš„ä¸€è‡´æ€§å’Œåˆç†æ€§

**æ£€æŸ¥é¡¹**:
```python
class ReflectionAgent:
    def validate(self, reasoning_result):
        checks = []
        
        # æ£€æŸ¥1: componentæ˜¯å¦åœ¨ç³»ç»Ÿæ‹“æ‰‘ä¸­å­˜åœ¨
        if reasoning_result.component not in system_components:
            checks.append("FAIL: componentä¸å­˜åœ¨")
        
        # æ£€æŸ¥2: reasoné•¿åº¦æ˜¯å¦è¶…è¿‡20è¯
        if len(reasoning_result.reason.split()) > 20:
            checks.append("WARN: reasonè¶…è¿‡20è¯ï¼Œå°†è¢«æˆªæ–­")
        
        # æ£€æŸ¥3: observationæ˜¯å¦åœ¨å‰20è¯åŒ…å«å…³é”®ä¿¡æ¯
        for step in reasoning_result.reasoning_trace:
            first_20_words = ' '.join(step.observation.split()[:20])
            if not self.contains_evidence(first_20_words):
                checks.append(f"WARN: Step {step.step} observationå‰20è¯ç¼ºå°‘å…³é”®è¯æ®")
        
        # æ£€æŸ¥4: æ¨ç†æ­¥æ•°æ˜¯å¦åˆç†ï¼ˆå»ºè®®3-7æ­¥ï¼‰
        if len(reasoning_result.reasoning_trace) > 10:
            checks.append("WARN: æ¨ç†æ­¥æ•°è¿‡å¤šï¼Œå½±å“Efficiencyå¾—åˆ†")
        
        # æ£€æŸ¥5: å„è¯æ®æºæ˜¯å¦ä¸€è‡´æŒ‡å‘åŒä¸€component
        components_mentioned = self.extract_components(reasoning_result)
        if len(set(components_mentioned)) > 1:
            checks.append("FAIL: ä¸åŒè¯æ®æŒ‡å‘ä¸åŒç»„ä»¶ï¼Œéœ€é‡æ–°åˆ†æ")
        
        return checks
```

---

## ğŸ’» æŠ€æœ¯å®ç°æ–¹æ¡ˆ

### 3.1 å¼€å‘ç¯å¢ƒä¸å·¥å…·æ ˆ

**æ ¸å¿ƒæŠ€æœ¯æ ˆ**:
```yaml
ç¼–ç¨‹è¯­è¨€: Python 3.10+
LLMæ¥å£: 
  - QWQ-LLaMA API (https://uni-api.cstcloud.cn)
  - DeepSeek-LLM API
  
æ¡†æ¶é€‰æ‹©:
  - Agentæ¡†æ¶: LangChain / AutoGen / CrewAI
  - æ•°æ®å¤„ç†: pandas, pyarrow (parquetæ–‡ä»¶)
  - æ—¶åºåˆ†æ: numpy, scipy
  - å›¾åˆ†æ: networkx
  - æ—¥å¿—è§£æ: drain3
  - å¼‚å¸¸æ£€æµ‹: scikit-learn (IsolationForest)

æ•°æ®æ ¼å¼:
  - è¾“å…¥: Parquetæ ¼å¼çš„ç›‘æ§æ•°æ®
  - è¾“å‡º: JSONæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆschemaï¼‰
```

### 3.2 ç›®å½•ç»“æ„è®¾è®¡

```
aiops-challenge-2025/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ llm_config.yaml          # LLM APIé…ç½®
â”‚   â”œâ”€â”€ metric_config.yaml       # å…³é”®æŒ‡æ ‡é…ç½®
â”‚   â””â”€â”€ system_topology.json     # æœåŠ¡æ‹“æ‰‘
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # åŸå§‹æ•°æ®
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â””â”€â”€ traces/
â”‚   â””â”€â”€ processed/               # å¤„ç†åæ•°æ®
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_refinement/
â”‚   â”‚   â”œâ”€â”€ metric_agent.py
â”‚   â”‚   â”œâ”€â”€ log_agent.py
â”‚   â”‚   â””â”€â”€ trace_agent.py
â”‚   â”œâ”€â”€ reasoning/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”‚   â”œâ”€â”€ metric_analysis_agent.py
â”‚   â”‚   â”œâ”€â”€ log_analysis_agent.py
â”‚   â”‚   â”œâ”€â”€ trace_analysis_agent.py
â”‚   â”‚   â”œâ”€â”€ rcl_agent.py         # Root Cause Localization
â”‚   â”‚   â””â”€â”€ reflection_agent.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ llm_client.py        # LLM APIå°è£…
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â””â”€â”€ evaluator.py         # æœ¬åœ°è¯„åˆ†æµ‹è¯•
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ metric_analysis.txt
â”‚   â”œâ”€â”€ log_analysis.txt
â”‚   â”œâ”€â”€ trace_analysis.txt
â”‚   â””â”€â”€ root_cause.txt
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_cases/              # ç”¨ä¾‹æµ‹è¯•
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ submissions/             # æäº¤ç»“æœ
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### 3.3 æ ¸å¿ƒä»£ç æ¡†æ¶

**LLM Clientå°è£…**:
```python
# src/utils/llm_client.py
import requests
from typing import Dict, List
import yaml

class LLMClient:
    def __init__(self, config_path: str):
        with open(config_path) as f:
            self.config = yaml.safe_load(f)
        self.api_url = self.config['api_url']
        self.api_key = self.config['api_key']
        self.model = self.config['model']  # "qwq-llama" or "deepseek"
    
    def call(self, prompt: str, temperature: float = 0.3, 
             max_tokens: int = 2000) -> str:
        """è°ƒç”¨LLM API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        response = requests.post(self.api_url, json=payload, headers=headers)
        return response.json()['choices'][0]['message']['content']
```

**Metric Refinementæ ¸å¿ƒé€»è¾‘**:
```python
# src/data_refinement/metric_agent.py
import pandas as pd
from typing import Dict, List, Tuple

class MetricAgent:
    def __init__(self, key_metrics: List[str]):
        self.key_metrics = key_metrics
    
    def extract_time_windows(self, fault_start: str, fault_end: str, 
                            history_faults: List) -> Dict:
        """åˆ’åˆ†æ­£å¸¸æœŸä¸æ•…éšœæœŸæ—¶é—´çª—å£"""
        fault_period = (fault_start, fault_end)
        
        # æ‰¾å‰åæ­£å¸¸æœŸï¼ˆé¿å¼€å…¶ä»–æ•…éšœ+10minç¼“å†²ï¼‰
        normal_before = self._find_normal_period_before(fault_start, history_faults)
        normal_after = self._find_normal_period_after(fault_end, history_faults)
        
        return {
            'fault': fault_period,
            'normal_before': normal_before,
            'normal_after': normal_after
        }
    
    def compute_statistics(self, df: pd.DataFrame, time_period: Tuple) -> Dict:
        """è®¡ç®—æŒ‡æ ‡çš„ç»Ÿè®¡ç‰¹å¾"""
        start, end = time_period
        data = df[(df['timestamp'] >= start) & (df['timestamp'] <= end)]
        
        # æ­£å¸¸æœŸå»é™¤æç«¯å€¼
        if 'normal' in str(time_period):
            data = data.sort_values()
            data = data.iloc[2:-2]  # å»é™¤æœ€å°2ä¸ªå’Œæœ€å¤§2ä¸ª
        
        stats = {
            'count': len(data),
            'mean': data.mean(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max(),
            'q1': data.quantile(0.25),
            'median': data.median(),
            'q3': data.quantile(0.75),
            'p95': data.quantile(0.95),
            'p99': data.quantile(0.99),
            'non_zero_ratio': (data != 0).sum() / len(data)
        }
        return stats
    
    def filter_anomalies(self, normal_stats: Dict, fault_stats: Dict, 
                        threshold: float = 0.05) -> bool:
        """åˆ¤æ–­æŒ‡æ ‡æ˜¯å¦å¼‚å¸¸ï¼ˆå¯¹ç§°æ¯”ç‡æ³•ï¼‰"""
        p50_ratio = abs(fault_stats['median'] - normal_stats['median']) / \
                    ((fault_stats['median'] + normal_stats['median'])/2 + 1e-9)
        p99_ratio = abs(fault_stats['p99'] - normal_stats['p99']) / \
                    ((fault_stats['p99'] + normal_stats['p99'])/2 + 1e-9)
        
        return p50_ratio >= threshold or p99_ratio >= threshold
    
    def process_service_metrics(self, service_name: str, 
                               time_windows: Dict) -> Dict:
        """å¤„ç†Serviceçº§æŒ‡æ ‡"""
        results = {}
        for metric in self.key_metrics:
            df = self.load_metric_data(service_name, metric)
            
            normal_stats = self.compute_statistics(df, time_windows['normal_before'])
            fault_stats = self.compute_statistics(df, time_windows['fault'])
            
            if self.filter_anomalies(normal_stats, fault_stats):
                results[metric] = {
                    'normal': normal_stats,
                    'fault': fault_stats,
                    'is_anomaly': True
                }
        
        return results
```

**Orchestratoræ ¸å¿ƒé€»è¾‘**:
```python
# src/reasoning/orchestrator.py
from typing import Dict, List
from .metric_analysis_agent import MetricAnalysisAgent
from .log_analysis_agent import LogAnalysisAgent
from .trace_analysis_agent import TraceAnalysisAgent
from .rcl_agent import RootCauseAgent
from .reflection_agent import ReflectionAgent

class OrchestratorAgent:
    def __init__(self, llm_client):
        self.llm = llm_client
        self.metric_agent = MetricAnalysisAgent(llm_client)
        self.log_agent = LogAnalysisAgent(llm_client)
        self.trace_agent = TraceAnalysisAgent(llm_client)
        self.rcl_agent = RootCauseAgent(llm_client)
        self.reflection = ReflectionAgent()
    
    def diagnose(self, uuid: str, query: str, refined_data: Dict) -> Dict:
        """ä¸»æ¨ç†æµç¨‹ï¼ˆæ§åˆ¶åœ¨5æ­¥å·¦å³ï¼‰"""
        reasoning_trace = []
        step_count = 1
        
        # Step 1: å¿«é€Ÿæ‰«æServiceçº§æŒ‡æ ‡å¼‚å¸¸
        metric_obs = self.metric_agent.analyze_service_level(
            refined_data['metrics']['service']
        )
        reasoning_trace.append({
            "step": step_count,
            "action": f"AnalyzeServiceMetrics({metric_obs['suspicious_services']})",
            "observation": self._truncate_20words(metric_obs['summary'])
        })
        step_count += 1
        
        # Step 2: æ ¹æ®æŒ‡æ ‡å¼‚å¸¸ç±»å‹é€‰æ‹©ä¸‹ä¸€æ­¥
        if 'error' in metric_obs['anomaly_type']:
            # æ£€æŸ¥æ—¥å¿—
            log_obs = self.log_agent.analyze(
                refined_data['logs'], 
                service=metric_obs['suspicious_services'][0]
            )
            reasoning_trace.append({
                "step": step_count,
                "action": f"AnalyzeLogs({log_obs['service']})",
                "observation": self._truncate_20words(log_obs['summary'])
            })
            step_count += 1
        
        if 'latency' in metric_obs['anomaly_type']:
            # æ£€æŸ¥è°ƒç”¨é“¾
            trace_obs = self.trace_agent.analyze(
                refined_data['traces'],
                service=metric_obs['suspicious_services'][0]
            )
            reasoning_trace.append({
                "step": step_count,
                "action": f"AnalyzeTraces('{trace_obs['path']}')",
                "observation": self._truncate_20words(trace_obs['summary'])
            })
            step_count += 1
        
        # Step 3: å¦‚æœç–‘ä¼¼åŸºç¡€è®¾æ–½é—®é¢˜ï¼Œä¸‹æ¢Node/Pod
        if metric_obs.get('infra_related'):
            infra_obs = self.metric_agent.analyze_infrastructure(
                refined_data['metrics']['infra'],
                pods=metric_obs['related_pods']
            )
            reasoning_trace.append({
                "step": step_count,
                "action": f"AnalyzeInfraMetrics({infra_obs['nodes']})",
                "observation": self._truncate_20words(infra_obs['summary'])
            })
            step_count += 1
        
        # Step 4: ç»¼åˆå®šä½æ ¹å› 
        evidence = {
            'metric': metric_obs,
            'log': log_obs if 'log_obs' in locals() else None,
            'trace': trace_obs if 'trace_obs' in locals() else None,
            'infra': infra_obs if 'infra_obs' in locals() else None
        }
        
        root_cause = self.rcl_agent.localize(evidence)
        
        # æœ€ç»ˆç»“æœ
        result = {
            "uuid": uuid,
            "component": root_cause['component'],
            "reason": self._truncate_20words(root_cause['reason']),
            "reasoning_trace": reasoning_trace
        }
        
        # Reflectionæ£€æŸ¥
        validation = self.reflection.validate(result)
        if validation['has_errors']:
            # é‡æ–°æ¨ç†ï¼ˆæœ€å¤š1æ¬¡ï¼‰
            print(f"Reflectionå‘ç°é—®é¢˜: {validation['errors']}, é‡æ–°åˆ†æ...")
            # ... é‡æ–°æ‰§è¡Œé€»è¾‘
        
        return result
    
    def _truncate_20words(self, text: str) -> str:
        """ç¡®ä¿æ–‡æœ¬ä¸è¶…è¿‡20è¯"""
        words = text.split()
        return ' '.join(words[:20])
```

---

## ğŸ“… æ•æ·è¿­ä»£è®¡åˆ’ï¼ˆMVPä¼˜å…ˆï¼‰

### è¿­ä»£æ€è·¯
- **Week 1**: è·‘é€šæœ€ç®€å•çš„baselineï¼ˆå•ä¸ªcaseï¼Œç¡¬ç¼–ç ä¹Ÿè¡Œï¼‰
- **Week 2**: æ³›åŒ–åˆ°æ‰€æœ‰caseï¼ˆè§„åˆ™+LLMï¼‰
- **Week 3**: ä¼˜åŒ–å‡†ç¡®ç‡ï¼ˆLA+TAï¼‰
- **Week 4**: ä¼˜åŒ–Efficiencyå’ŒExplainability

### Iteration 0: MVP Baselineï¼ˆ3å¤©ï¼‰

**ç›®æ ‡**: ç”¨æœ€ç®€å•çš„æ–¹æ³•è·‘é€šä¸€ä¸ªcaseï¼Œäº†è§£è¯„åˆ†æœºåˆ¶

```python
# mvp.py - 50è¡Œä»£ç æå®š
def mvp_solution(uuid, data):
    # 1. æ‰¾å¼‚å¸¸æœ€ä¸¥é‡çš„ç»„ä»¶ï¼ˆç®€å•ç®—ä¸ªå‡å€¼æ–¹å·®ï¼‰
    component = max(data['services'], key=lambda s: anomaly_score(s))
    
    # 2. ä»è¯æ®ä¸­æ‹¼æ¥reasonï¼ˆç¡¬ç¼–ç ä¹Ÿè¡Œï¼‰
    reason = f"high error rate, metrics: error_ratio"
    
    # 3. å›ºå®š3æ­¥æ¨ç†é“¾
    reasoning_trace = [
        {"step": 1, "action": "Check metrics", "observation": "error_ratio spike"},
        {"step": 2, "action": "Check logs", "observation": "errors found"},
        {"step": 3, "action": "Confirm", "observation": f"{component} is root cause"}
    ]
    
    return {"uuid": uuid, "component": component, "reason": reason, "reasoning_trace": reasoning_trace}
```

**éªŒè¯ç‚¹**: èƒ½å¦è·å¾—>10åˆ†ï¼Ÿäº†è§£å“ªé‡Œæ‰£åˆ†ï¼Ÿ

### Iteration 1: é¢„å¤„ç†ç®¡é“ï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**: å®ç°Pass 0ï¼Œäº§å‡ºé«˜è´¨é‡å€™é€‰åˆ—è¡¨

**ä¼˜å…ˆçº§æ’åº**:
1. P0: Metricå¼‚å¸¸æ£€æµ‹ï¼ˆ3-sigma + å˜åŒ–ç‡ï¼‰
2. P1: æ—¥å¿—ERRORè¿‡æ»¤
3. P2: Traceå¼‚å¸¸æ£€æµ‹ï¼ˆoptionalï¼Œæ—¶é—´ä¸å¤Ÿå¯è·³è¿‡ï¼‰

**äº¤ä»˜**: `preprocessor.py` - è¾“å…¥raw dataï¼Œè¾“å‡ºTop-5å€™é€‰+è¯æ®

**éªŒè¯**: äººå·¥æ ‡æ³¨10ä¸ªcaseï¼Œçœ‹çœŸå®æ ¹å› æ˜¯å¦åœ¨Top-5å†…ï¼ˆç›®æ ‡å¬å›ç‡>90%ï¼‰

### Iteration 2: LLMé›†æˆï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**: å®ç°Pass 1 + Pass 2çš„Prompt

**å…³é”®ä»»åŠ¡**:
1. è®¾è®¡Pass 1çš„Promptï¼ˆæ ¹å› é€‰æ‹©ï¼‰
2. è®¾è®¡Pass 2çš„Promptï¼ˆæ¨ç†é“¾ç”Ÿæˆï¼‰
3. å®ç°åå¤„ç†ï¼ˆå…³é”®è¯æ³¨å…¥ã€20è¯æˆªæ–­ï¼‰

**äº¤ä»˜**: `llm_reasoner.py` + 2ä¸ªPromptæ¨¡æ¿

**éªŒè¯**: åœ¨10ä¸ªcaseä¸Šæµ‹LAå’ŒTAå¾—åˆ†ï¼ˆç›®æ ‡LA>0.5, TA>0.4ï¼‰

### Iteration 3: ä¼˜åŒ–ä¸è°ƒä¼˜ï¼ˆ1å‘¨ï¼‰

**ä¼˜åŒ–æ–¹å‘**:

| æŒ‡æ ‡ä½äºé¢„æœŸ | è¯Šæ–­ | ä¼˜åŒ–æ–¹æ¡ˆ |
|------------|------|---------|
| LA < 0.6 | é¢„å¤„ç†å¬å›ä¸è¶³ | è°ƒæ•´å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ã€å¢åŠ å€™é€‰æ•° |
| TA < 0.5 | å…³é”®è¯æœªå‘½ä¸­ | æ‰©å……å…³é”®è¯åº“ã€æ£€æŸ¥Prompt |
| Efficiency < 0.8 | æ­¥æ•°è¿‡å¤š | å›ºå®š3æ­¥æ¨¡æ¿ |
| Explainability < 0.7 | observationä¸è¾¾æ ‡ | å¼ºåˆ¶å…³é”®è¯å‰ç½®åˆ°å‰20è¯ |

**äº¤ä»˜**: è°ƒä¼˜åçš„ç‰ˆæœ¬ï¼Œåœ¨50ä¸ªcaseä¸Šæµ‹è¯•

### Iteration 4: å¤§è§„æ¨¡æµ‹è¯•ï¼ˆ3å¤©ï¼‰

**ä»»åŠ¡**:
1. åœ¨å…¨é‡æµ‹è¯•é›†ä¸Šè¿è¡Œ
2. åˆ†æbadcaseï¼ˆå“ªäº›ç±»å‹é”™å¾—å¤šï¼Ÿï¼‰
3. é’ˆå¯¹æ€§ä¿®å¤

**äº¤ä»˜**: æœ€ç»ˆæäº¤æ–‡ä»¶ + badcaseåˆ†ææŠ¥å‘Š

---

---

## ğŸ¯ æ ¸å¿ƒç«äº‰ç­–ç•¥æ€»ç»“

### ç­–ç•¥çŸ©é˜µ

| è¯„åˆ†ç»´åº¦ | ä¼ ç»Ÿåšæ³• | æœ¬æ–¹æ¡ˆåˆ›æ–° | é¢„æœŸæå‡ |
|---------|---------|-----------|---------|
| **LA (40%)** | LLMè‡ªç”±æ¨ç† | **é¢„å¤„ç†Top-5å€™é€‰+LLMé€‰æ‹©** | +20% |
| **TA (40%)** | LLMç”Ÿæˆæè¿° | **å…³é”®è¯åº“çˆ†ç ´+å¼ºåˆ¶æ³¨å…¥** | +30% |
| **Efficiency (10%)** | å¤šAgentæ¢ç´¢ | **å›ºå®š3æ­¥æ¨¡æ¿** | æ»¡åˆ† |
| **Explainability (10%)** | LLMè‡ªç”±å‘æŒ¥ | **åå¤„ç†å¼ºåˆ¶å‰20è¯ä¼˜åŒ–** | æ»¡åˆ† |

### ä¸ç°æœ‰æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | LLMè°ƒç”¨æ¬¡æ•° | æ¨ç†æ­¥æ•° | LAå‡†ç¡®ç‡ | TAå‡†ç¡®ç‡ | å¼€å‘å¤æ‚åº¦ |
|------|-----------|---------|---------|---------|----------|
| MicroRCA-Agent | 5-10æ¬¡ | ä¸å›ºå®š | ä¸­ | ä¸­ | é«˜ |
| TVDiag | 2-3æ¬¡ | ä¸å›ºå®š | ä¸­-é«˜ | ä½ï¼ˆæ— å…³é”®è¯ä¼˜åŒ–ï¼‰ | ä¸­ |
| **æœ¬æ–¹æ¡ˆ** | **2æ¬¡** | **å›ºå®š3æ­¥** | **é«˜** | **é«˜** | **ä½** |

---

## ğŸ” å…³é”®ä¼˜åŒ–ç‚¹

### ä¼˜åŒ–1: Reasonå…³é”®è¯åµŒå…¥ç­–ç•¥

**é—®é¢˜**: TAè¯„åˆ†ä¾èµ–å…³é”®è¯åŒ¹é…ï¼Œå¿…é¡»ç¡®ä¿reasonåŒ…å«ground truthå…³é”®è¯

**è§£å†³æ–¹æ¡ˆ**:
1. åœ¨Metric Agentè¾“å‡ºä¸­å¼ºåˆ¶åŒ…å«æŒ‡æ ‡åç§°ï¼ˆå¦‚`disk_read_latency`ï¼‰
2. åœ¨Log Agentè¾“å‡ºä¸­ä¿ç•™æ—¥å¿—å…³é”®è¯ï¼ˆå¦‚`IOError`ï¼‰
3. åœ¨RCL Agent promptä¸­æ˜ç¡®è¦æ±‚ï¼š
   ```
   ã€é‡è¦ã€‘reasonå¿…é¡»åŒ…å«ä»¥ä¸‹ç±»å‹çš„å…³é”®è¯ï¼š
   - å…·ä½“æŒ‡æ ‡åç§°ï¼ˆå¦‚node_network_transmit_bytes_totalï¼‰
   - æ—¥å¿—å…³é”®è¯ï¼ˆå¦‚IOError, timeoutï¼‰
   - èµ„æºç±»å‹ï¼ˆå¦‚disk, memory, CPU, networkï¼‰
   ```

### ä¼˜åŒ–2: Observationå‰20è¯ç­–ç•¥

**é—®é¢˜**: Explainabilityè¯„åˆ†åªçœ‹observationå‰20è¯

**è§£å†³æ–¹æ¡ˆ**:
1. æ¨¡æ¿åŒ–observationæ ¼å¼ï¼š
   ```
   <å…³é”®æŒ‡æ ‡å> <å¼‚å¸¸æ–¹å‘> <æ—¶é—´ç‚¹/å¹…åº¦>, <è¡¥å……ä¿¡æ¯>
   
   ç¤ºä¾‹:
   "disk_read_latency spike observed at 12:18, 200% increase from baseline"
   (å‰20è¯: disk_read_latency spike observed at 12:18 200 increase from baseline)
   ```

2. åœ¨Orchestratorä¸­å®ç°è‡ªåŠ¨æ£€æŸ¥ï¼š
   ```python
   def ensure_evidence_in_first_20(observation: str, evidence_keywords: List[str]):
       first_20 = ' '.join(observation.split()[:20])
       for kw in evidence_keywords:
           if kw not in first_20:
               # é‡æ–°ç»„ç»‡observationï¼ŒæŠŠå…³é”®è¯å‰ç½®
               observation = f"{kw} {observation}"
       return observation
   ```

### ä¼˜åŒ–3: æ¨ç†æ­¥æ•°æ§åˆ¶

**ç›®æ ‡**: APLæ¥è¿‘5æ­¥ä»¥è·å¾—æœ€ä½³Efficiencyå¾—åˆ†

**ç­–ç•¥**:
- å¯å‘å¼å†³ç­–æ ‘é¿å…ç›²ç›®æ¢ç´¢ï¼š
  ```
  IF æŒ‡æ ‡æ˜¾ç¤ºerror_ratioé«˜ THEN ç›´æ¥æŸ¥æ—¥å¿—ï¼ˆ2æ­¥ï¼‰
  ELSE IF æŒ‡æ ‡æ˜¾ç¤ºlatencyé«˜ THEN æŸ¥è°ƒç”¨é“¾ï¼ˆ2æ­¥ï¼‰
  ...
  æ€»è®¡ä¸è¶…è¿‡5æ­¥
  ```
- é¿å…"éå†æ‰€æœ‰æœåŠ¡"çš„ç­–ç•¥
- ä½¿ç”¨åˆ†å±‚åˆ†æï¼ˆService â†’ Pod â†’ Nodeï¼‰è€Œéå¹³è¡Œåˆ†æ

### ä¼˜åŒ–4: Componentç²¾ç¡®åŒ¹é…

**é—®é¢˜**: componentå¿…é¡»ä¸¥æ ¼åŒ¹é…ï¼ˆ"emailservice" â‰  "emailservice-0"ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
1. ç»´æŠ¤ç³»ç»Ÿæ‹“æ‰‘é…ç½® `config/system_topology.json`ï¼š
   ```json
   {
     "services": ["adservice", "checkoutservice", ...],
     "pods": {
       "adservice": ["adservice-0", "adservice-1", "adservice-2"],
       ...
     },
     "nodes": ["node-0", "node-1", ..., "node-7"]
   }
   ```

2. åœ¨RCL Agent promptä¸­æä¾›å€™é€‰åˆ—è¡¨ï¼š
   ```
   ã€å¯ç”¨ç»„ä»¶åˆ—è¡¨ã€‘
   Services: adservice, cartservice, checkoutservice, ...
   Pods: adservice-0, adservice-1, ...
   Nodes: node-0, node-1, ...
   
   è¯·ä»ä¸Šè¿°åˆ—è¡¨ä¸­é€‰æ‹©ç²¾ç¡®åŒ¹é…çš„ç»„ä»¶åç§°ã€‚
   ```

3. åå¤„ç†éªŒè¯ï¼š
   ```python
   if result['component'] not in valid_components:
       # å°è¯•æ¨¡ç³ŠåŒ¹é…å¹¶ä¿®æ­£
       result['component'] = fuzzy_match(result['component'], valid_components)
   ```

---

## ï¿½ å¸¸è§é™·é˜±ä¸é¿å‘æŒ‡å—

### é™·é˜±1: è¿‡åº¦ä¾èµ–LLM
âŒ **é”™è¯¯åšæ³•**: è®©LLMä»é›¶å¼€å§‹åˆ†ææµ·é‡åŸå§‹æ•°æ®  
âœ… **æ­£ç¡®åšæ³•**: é¢„å¤„ç†90%å™ªéŸ³ï¼ŒLLMåªåšæœ€åçš„å†³ç­–

### é™·é˜±2: å¿½è§†è¯„åˆ†ç»†èŠ‚
âŒ **é”™è¯¯åšæ³•**: reason = "The root cause is disk IO problem"  
âœ… **æ­£ç¡®åšæ³•**: reason = "disk IO overload, metrics: disk_read_latency, IOError"  
ï¼ˆåè€…èƒ½å‘½ä¸­å…³é”®è¯ï¼Œå‰è€…ä¸èƒ½ï¼‰

### é™·é˜±3: æ¨ç†é“¾è¿‡äºå¤æ‚
âŒ **é”™è¯¯åšæ³•**: 10æ­¥æ¢ç´¢å¼æ¨ç†ï¼ˆEfficiency=0.13ï¼‰  
âœ… **æ­£ç¡®åšæ³•**: 3æ­¥å›ºå®šæ¨¡æ¿ï¼ˆEfficiency=1.0ï¼‰

### é™·é˜±4: ObservationåŸ‹å…³é”®è¯
âŒ **é”™è¯¯åšæ³•**: "After analyzing the metrics and logs, I found that disk_read_latency increased significantly"ï¼ˆå…³é”®è¯åœ¨ç¬¬11è¯ï¼‰  
âœ… **æ­£ç¡®åšæ³•**: "disk_read_latency spike observed, 20x increase at 12:18"ï¼ˆå…³é”®è¯åœ¨å‰3è¯ï¼‰

### é™·é˜±5: Componentåç§°ä¸ç²¾ç¡®
âŒ **é”™è¯¯åšæ³•**: component = "checkout service"ï¼ˆæœ‰ç©ºæ ¼ï¼‰  
âœ… **æ­£ç¡®åšæ³•**: component = "checkoutservice"ï¼ˆä»ç³»ç»Ÿæ‹“æ‰‘ä¸­ç²¾ç¡®åŒ¹é…ï¼‰

---

## ğŸ“ è¿›é˜¶ä¼˜åŒ–æ–¹å‘ï¼ˆæ—¶é—´å……è£•æ—¶ï¼‰

### 1. å†å²æ¡ˆä¾‹æ£€ç´¢ï¼ˆç±»ä¼¼RAGï¼‰
```python
# æ„å»ºæ•…éšœæ¡ˆä¾‹åº“
case_db = {
    "disk_io_overload": [
        {"component": "checkoutservice", "keywords": ["disk_read_latency", "IOError"]},
        {"component": "node-3", "keywords": ["node_disk_write_time"]},
    ],
    # ...
}

# æ¨ç†æ—¶æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹
similar_cases = search_similar(current_evidence, case_db)
confidence += 0.2 if similar_cases else 0
```

### 2. å¤šæ¨¡å‹é›†æˆ
```python
# åŒæ—¶è°ƒç”¨QWQå’ŒDeepSeek
result1 = qwq_llm.call(prompt)
result2 = deepseek_llm.call(prompt)

# æŠ•ç¥¨æˆ–ç½®ä¿¡åº¦åŠ æƒ
if result1['component'] == result2['component']:
    final_result = result1
    final_result['confidence'] = 0.9
else:
    final_result = max([result1, result2], key=lambda x: evidence_support(x))
```

### 3. åœ¨çº¿å­¦ä¹ ï¼ˆæŒç»­ä¼˜åŒ–ï¼‰
```python
# æ¯æ¬¡æäº¤åæ ¹æ®çœŸå®å¾—åˆ†è°ƒæ•´
if LA_score < 0.5:
    # è°ƒæ•´é¢„å¤„ç†çš„å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
    ANOMALY_THRESHOLD *= 0.9
    
if TA_score < 0.5:
    # æ‰©å……å…³é”®è¯åº“
    update_keyword_bank_from_badcases()
```

---

## ğŸ“Š æ•ˆæœé¢„ä¼°ï¼ˆåŸºäºç­–ç•¥åˆ†æï¼‰

### ä¿å®ˆä¼°è®¡ï¼ˆä¸‹é™ï¼‰
- LA: 0.65ï¼ˆé¢„å¤„ç†Top-5å¬å›ç‡90% Ã— LLMé€‰æ‹©å‡†ç¡®ç‡70%ï¼‰
- TA: 0.60ï¼ˆå…³é”®è¯å‘½ä¸­ç‡50% + è¯­ä¹‰ç›¸ä¼¼åº¦0.7Ã—50%ï¼‰
- Efficiency: 0.95ï¼ˆå›ºå®š3æ­¥ï¼‰
- Explainability: 0.80ï¼ˆæ¨¡æ¿åŒ–ä¿è¯ï¼‰
- **Final Score: 68**

### ä¹è§‚ä¼°è®¡ï¼ˆä¸Šé™ï¼‰
- LA: 0.80ï¼ˆä¼˜åŒ–åé¢„å¤„ç†å¬å›ç‡95% Ã— LLMå‡†ç¡®ç‡85%ï¼‰
- TA: 0.75ï¼ˆå…³é”®è¯åº“å…¨é¢+Promptä¼˜åŒ–ï¼‰
- Efficiency: 1.00ï¼ˆå›ºå®š3æ­¥ï¼‰
- Explainability: 0.90ï¼ˆåå¤„ç†ä¼˜åŒ–ï¼‰
- **Final Score: 82**

---

## ğŸ“ è¡ŒåŠ¨æ¸…å•ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### æœ¬å‘¨å¿…åšï¼ˆP0ï¼‰
- [ ] ä¸‹è½½æ¯”èµ›æ•°æ®ï¼Œè·‘ä¸€éæ•°æ®æ¢ç´¢ï¼ˆäº†è§£æ•°æ®æ ¼å¼ï¼‰
- [ ] å®ç°MVP baselineï¼ˆ50è¡Œä»£ç ï¼Œå“ªæ€•ç¡¬ç¼–ç ï¼‰
- [ ] ç”³è¯·LLM Tokenå¹¶æµ‹è¯•API
- [ ] æ‰‹å·¥æ ‡æ³¨10ä¸ªcaseï¼ˆç†è§£ground truthç‰¹å¾ï¼‰

### ä¸‹å‘¨ç›®æ ‡ï¼ˆP1ï¼‰
- [ ] å®ç°é¢„å¤„ç†ç®¡é“ï¼ˆå¼‚å¸¸æ£€æµ‹+å€™é€‰æ’åºï¼‰
- [ ] æ„å»ºå…³é”®è¯åº“ï¼ˆä»æ•°æ®ä¸­è‡ªåŠ¨æå–ï¼‰
- [ ] è®¾è®¡å¹¶æµ‹è¯•Pass 1çš„Prompt

### ä¸¤å‘¨åï¼ˆP2ï¼‰
- [ ] å®Œæˆç«¯åˆ°ç«¯æµç¨‹
- [ ] åœ¨50ä¸ªcaseä¸Šæµ‹è¯•å¹¶è°ƒä¼˜
- [ ] å®ç°åå¤„ç†ä¼˜åŒ–ï¼ˆå…³é”®è¯æ³¨å…¥ã€20è¯æˆªæ–­ï¼‰

### æœ€åä¸€å‘¨ï¼ˆP3ï¼‰
- [ ] å¤§è§„æ¨¡æµ‹è¯•ï¼ˆå…¨é‡æ•°æ®ï¼‰
- [ ] Badcaseåˆ†æä¸ä¿®å¤
- [ ] å‡†å¤‡æäº¤æ–‡ä»¶

---

## ğŸ’¡ æœ€åçš„å»ºè®®

1. **å…ˆæ±‚ç¨³å†æ±‚å¿«**: LAå’ŒTAæ˜¯80%çš„åˆ†æ•°ï¼ŒEfficiencyå’ŒExplainabilityåªæ˜¯é”¦ä¸Šæ·»èŠ±
2. **æ•°æ®>æ¨¡å‹**: é¢„å¤„ç†è´¨é‡å†³å®šä¸Šé™ï¼ŒLLMåªæ˜¯ä¸´é—¨ä¸€è„š
3. **æµ‹è¯•é©±åŠ¨**: æ¯æ”¹ä¸€å¤„ä»£ç ï¼Œç«‹å³åœ¨å°æ‰¹é‡caseä¸Šæµ‹è¯•
4. **å…³é”®è¯ä¸ºç‹**: TAè¯„åˆ†çš„å…³é”®æ˜¯å…³é”®è¯å‘½ä¸­ï¼Œä¸æ˜¯è¯­ä¹‰ç†è§£
5. **æ¨¡æ¿åŒ–è¾“å‡º**: ä¸è¦è®©LLMè‡ªç”±å‘æŒ¥ï¼Œç”¨æ¨¡æ¿çº¦æŸæ ¼å¼

---

**é¢„ç¥æˆåŠŸï¼** ğŸ‰

å¦‚æœ‰é—®é¢˜éšæ—¶è®¨è®ºè°ƒæ•´ç­–ç•¥ã€‚è®°ä½ï¼š**ç®€å•çš„æ–¹æ¡ˆå¾€å¾€æ¯”å¤æ‚çš„æ–¹æ¡ˆæ›´æœ‰æ•ˆ**ã€‚
